{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import mxnet\n",
    "from data_utilities import load_mnist\n",
    "import mx_layers as layers\n",
    "from mx_utility import mxnet_array_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stretched = load_mnist('stretched_mnist/', scale=1.0, shape=(1, 56, 56))\n",
    "stretched_canvas = load_mnist('stretched_canvas_mnist/', scale=1.0, shape=(1, 56, 56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_residual_layers = 3\n",
    "postfix = 'round0'\n",
    "identifier = 'shrinked-mnist-residual-network-%d-%s' % (n_residual_layers, postfix)\n",
    "parameters, states = pickle.load(open('parameters/%s' % identifier, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _normalized_convolution(**args):\n",
    "  network = layers.convolution(**args)\n",
    "  network = layers.batch_normalization(network)\n",
    "  network = layers.ReLU(network)\n",
    "  return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = layers.variable('data')\n",
    "\n",
    "for index in range(3):\n",
    "  network = _normalized_convolution(X=network, n_filters=16, kernel_shape=(5, 5), stride=(1, 1), pad=(2, 2))\n",
    "  network = layers.pooling(X=network, mode='maximum', kernel_shape=(2, 2), stride=(2, 2), pad=(0, 0))\n",
    "\n",
    "shared_weight = layers.variable('shared_convolution_weight')\n",
    "shared_bias = layers.variable('shared_convolution_bias')\n",
    "kwargs = {'n_filters' : 16, 'kernel_shape' : (3, 3), 'stride' : (1, 1), 'pad' : (1, 1)}\n",
    "\n",
    "for index in range(n_residual_layers):\n",
    "  identity = network\n",
    "  residual = _normalized_convolution(X=network, weight=shared_weight, bias=shared_bias, **kwargs)\n",
    "  network = identity + residual\n",
    "\n",
    "network = layers.pooling(X=network, mode='average', global_pool=True, kernel_shape=(1, 1), stride=(1, 1), pad=(0, 0))\n",
    "network = layers.flatten(network)\n",
    "network = layers.fully_connected(X=network, n_hidden_units=10)\n",
    "# network = layers.softmax_loss(prediction=network, normalization='batch', id='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 64\n",
    "args = {'data' : stretched[0][:N]}\n",
    "args.update(parameters)\n",
    "args = mxnet_array_mapping(args)\n",
    "aux_states = mxnet_array_mapping(states)\n",
    "executor = network.bind(mxnet.cpu(), args, aux_states=aux_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolution1_weight\n",
      "shared_convolution_bias\n",
      "convolution0_bias\n",
      "convolution1_bias\n",
      "convolution0_weight\n",
      "convolution2_bias\n",
      "convolution2_weight\n",
      "shared_convolution_weight\n"
     ]
    }
   ],
   "source": [
    "for key in parameters.keys():\n",
    "    if 'convolution' in key: print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method bind in module mxnet.symbol:\n",
      "\n",
      "bind(self, ctx, args, args_grad=None, grad_req='write', aux_states=None, group2ctx=None, shared_exec=None) method of mxnet.symbol.Symbol instance\n",
      "    Bind current symbol to get an executor.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    ctx : Context\n",
      "        The device context the generated executor to run on.\n",
      "    \n",
      "    args : list of NDArray or dict of str to NDArray\n",
      "        Input arguments to the symbol.\n",
      "    \n",
      "        - If type is list of NDArray, the position is in the same order of list_arguments.\n",
      "        - If type is dict of str to NDArray, then it maps the name of arguments\n",
      "          to the corresponding NDArray.\n",
      "        - In either case, all the arguments must be provided.\n",
      "    \n",
      "    args_grad : list of NDArray or dict of str to NDArray, optional\n",
      "        When specified, args_grad provide NDArrays to hold\n",
      "        the result of gradient value in backward.\n",
      "    \n",
      "        - If type is list of NDArray, the position is in the same order of list_arguments.\n",
      "        - If type is dict of str to NDArray, then it maps the name of arguments\n",
      "          to the corresponding NDArray.\n",
      "        - When the type is dict of str to NDArray, users only need to provide the dict\n",
      "          for needed argument gradient.\n",
      "          Only the specified argument gradient will be calculated.\n",
      "    \n",
      "    grad_req : {'write', 'add', 'null'}, or list of str or dict of str to str, optional\n",
      "        Specifies how we should update the gradient to the args_grad.\n",
      "    \n",
      "        - 'write' means everytime gradient is write to specified args_grad NDArray.\n",
      "        - 'add' means everytime gradient is add to the specified NDArray.\n",
      "        - 'null' means no action is taken, the gradient may not be calculated.\n",
      "    \n",
      "    aux_states : list of NDArray, or dict of str to NDArray, optional\n",
      "        Input auxiliary states to the symbol, only need to specify when\n",
      "        list_auxiliary_states is not empty.\n",
      "    \n",
      "        - If type is list of NDArray, the position is in the same order of list_auxiliary_states\n",
      "        - If type is dict of str to NDArray, then it maps the name of auxiliary_states\n",
      "          to the corresponding NDArray,\n",
      "        - In either case, all the auxiliary_states need to be provided.\n",
      "    \n",
      "    group2ctx : dict of string to mx.Context\n",
      "        The dict mapping the ``ctx_group`` attribute to the context assignment.\n",
      "    \n",
      "    shared_exec : mx.executor.Executor\n",
      "        Executor to share memory with. This is intended for runtime reshaping, variable length\n",
      "        sequences, etc. The returned executor shares state with shared_exec, and should not be\n",
      "        used in parallel with it.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    executor : Executor\n",
      "        The generated Executor\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Auxiliary states are special states of symbols that do not corresponds to an argument,\n",
      "    and do not have gradient. But still be useful for the specific operations.\n",
      "    A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm.\n",
      "    Most operators do not have auxiliary states and this parameter can be safely ignored.\n",
      "    \n",
      "    User can give up gradient by using a dict in args_grad and only specify\n",
      "    gradient they interested in.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(network.bind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
